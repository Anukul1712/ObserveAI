{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13959815,"sourceType":"datasetVersion","datasetId":8898544},{"sourceId":13945413,"sourceType":"datasetVersion","datasetId":8888123}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!/usr/bin/env python3\n# train_combined_deberta_fixed_imports.py\n\nimport os\nimport json\nimport random\nimport difflib\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import AdamW\nfrom transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\nfrom sklearn.metrics import accuracy_score\nfrom tqdm.auto import tqdm\n\n# -----------------------\n# CONFIG - edit if needed\n# -----------------------\nMODEL_NAME = \"microsoft/deberta-v3-base\"\nAGENT_FILE = \"/kaggle/input/primary-secondary-data-with-general/labeled_intent_dataset_agent_with_general.json\"\nCUSTOMER_FILE = \"/kaggle/input/primary-secondary-data-with-general/labeled_intent_dataset_customer_with_general.json\"\nTAXONOMY_PATH = \"/kaggle/input/taxonomy-json/taxonomy.json\"\nOUTPUT_DIR = \"/kaggle/working/deberta_debug_model_fixed\"\n\nBATCH_SIZE = 16\nMAX_LEN = 128\nLR = 2e-5\nEPOCHS = 4\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nSEED = 42\nALPHA_SEC = 1.0  # weight for secondary loss\nWARMUP_STEPS = 100\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\n\n# -----------------------\n# Helpers\n# -----------------------\ndef load_json_list(path: str) -> List[dict]:\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n    if not isinstance(data, list):\n        raise ValueError(f\"{path} must contain a JSON array (list of objects).\")\n    return data\n\ndef normalize_str(s):\n    if s is None:\n        return \"\"\n    return str(s).strip()\n\ndef fuzzy_match_one(item: str, choices: List[str], cutoff: float = 0.6) -> str:\n    item_s = normalize_str(item)\n    if not item_s:\n        return None\n    # exact match\n    for c in choices:\n        if item_s == c:\n            return c\n    # case-insensitive exact\n    for c in choices:\n        if item_s.lower() == c.lower():\n            return c\n    # fuzzy via difflib\n    matches = difflib.get_close_matches(item_s, choices, n=1, cutoff=cutoff)\n    if matches:\n        return matches[0]\n    # also try lowercased choices\n    matches = difflib.get_close_matches(item_s.lower(), [c.lower() for c in choices], n=1, cutoff=cutoff)\n    if matches:\n        # find original-cased candidate\n        for c in choices:\n            if c.lower() == matches[0]:\n                return c\n    return None\n\n# -----------------------\n# Load taxonomy & build label maps\n# -----------------------\nwith open(TAXONOMY_PATH, \"r\", encoding=\"utf-8\") as f:\n    taxonomy = json.load(f)\n\nprimary_labels = sorted(list(taxonomy.keys()))\nprimary2id = {p: i for i, p in enumerate(primary_labels)}\nid2primary = {i: p for p, i in primary2id.items()}\n\nsec_agent = sorted({s for v in taxonomy.values() for s in v.get(\"agent\", [])})\nsec_customer = sorted({s for v in taxonomy.values() for s in v.get(\"user\", [])})\nall_secondary = sorted(list(set(sec_agent + sec_customer)))\nsec2id = {s: i for i, s in enumerate(all_secondary)}\nid2sec = {i: s for s, i in sec2id.items()}\n\n# allowed_map: (primary_id, speaker) -> list of allowed secondary ids\nallowed_map: Dict[Tuple[int, str], List[int]] = {}\nfor p_name, p_id in primary2id.items():\n    a_children = taxonomy[p_name].get(\"agent\", [])\n    u_children = taxonomy[p_name].get(\"user\", [])\n    allowed_map[(p_id, \"agent\")] = [sec2id[s] for s in a_children if s in sec2id]\n    allowed_map[(p_id, \"customer\")] = [sec2id[s] for s in u_children if s in sec2id]\n\n# -----------------------\n# Load raw data files\n# -----------------------\nagent_raw = load_json_list(AGENT_FILE)\ncustomer_raw = load_json_list(CUSTOMER_FILE)\nprint(\"Loaded agent:\", len(agent_raw), \"customer:\", len(customer_raw))\n\n# -----------------------\n# Fuzzy map primary & secondary labels to taxonomy (best-effort)\n# -----------------------\ndef map_records(records: List[dict]) -> List[dict]:\n    mapped = []\n    for r in records:\n        rr = dict(r)  # copy\n        prim = normalize_str(rr.get(\"primary_intent\") or \"\")\n        sec = normalize_str(rr.get(\"secondary_intent\") or \"\")\n        sp = normalize_str(rr.get(\"speaker\") or rr.get(\"original_speaker\") or \"\").lower()\n\n        # map primary\n        mapped_primary = fuzzy_match_one(prim, primary_labels, cutoff=0.6)\n        if mapped_primary:\n            rr[\"primary_intent\"] = mapped_primary\n        # map secondary from relevant pool\n        pool = sec_agent if sp == \"agent\" else sec_customer\n        mapped_secondary = fuzzy_match_one(sec, pool, cutoff=0.55)\n        if mapped_secondary:\n            rr[\"secondary_intent\"] = mapped_secondary\n\n        # normalize speaker\n        if sp.startswith(\"agent\"):\n            rr[\"speaker\"] = \"agent\"\n        elif sp.startswith(\"cust\") or sp.startswith(\"customer\") or sp.startswith(\"user\"):\n            rr[\"speaker\"] = \"customer\"\n        else:\n            # keep provided speaker lowercased (fallback)\n            rr[\"speaker\"] = sp if sp else rr.get(\"speaker\",\"\").lower() or \"customer\"\n        mapped.append(rr)\n    return mapped\n\nagent_mapped = map_records(agent_raw)\ncustomer_mapped = map_records(customer_raw)\n\n# -----------------------\n# Filter valid records\n# -----------------------\ndef is_valid_record(r):\n    return (\n        (r.get(\"primary_intent\") in primary2id)\n        and (r.get(\"secondary_intent\") in sec2id)\n        and (str(r.get(\"speaker\",\"\")).lower() in (\"agent\",\"customer\"))\n        and ( (r.get(\"text\") or r.get(\"full_text\") or \"\").strip() != \"\" )\n    )\n\nagent_valid = [r for r in agent_mapped if is_valid_record(r)]\ncustomer_valid = [r for r in customer_mapped if is_valid_record(r)]\nall_records = agent_valid + customer_valid\nrandom.shuffle(all_records)\nprint(\"Valid records -> agent:\", len(agent_valid), \"customer:\", len(customer_valid), \"total:\", len(all_records))\n\nif len(all_records) == 0:\n    raise RuntimeError(\"No valid records after mapping/filtering. Check file contents, keys 'primary_intent' and 'secondary_intent', and taxonomy strings.\")\n\n# -----------------------\n# Tokenizer & Dataset\n# -----------------------\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n\nclass IntentDataset(Dataset):\n    def __init__(self, records: List[dict], tokenizer, max_len=128):\n        self.records = records\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.records)\n\n    def __getitem__(self, idx):\n        r = self.records[idx]\n        text = r.get(\"text\") or r.get(\"full_text\") or \"\"\n        enc = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n        primary_id = primary2id.get(r.get(\"primary_intent\"), -1)\n        secondary_id = sec2id.get(r.get(\"secondary_intent\"), -1)\n        return {\n            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n            \"speaker\": r.get(\"speaker\",\"\").lower(),\n            \"primary_id\": primary_id,\n            \"secondary_id\": secondary_id\n        }\n\ndef collate_fn(batch):\n    input_ids = torch.stack([b[\"input_ids\"] for b in batch])\n    attention_mask = torch.stack([b[\"attention_mask\"] for b in batch])\n    speakers = [b[\"speaker\"] for b in batch]\n    primaries = torch.tensor([b[\"primary_id\"] for b in batch], dtype=torch.long)\n    secondaries = torch.tensor([b[\"secondary_id\"] for b in batch], dtype=torch.long)\n    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"speakers\": speakers, \"primaries\": primaries, \"secondaries\": secondaries}\n\n# train/val split\nsplit_idx = int(0.8 * len(all_records))\ntrain_records = all_records[:split_idx]\nval_records = all_records[split_idx:]\n\ntrain_ds = IntentDataset(train_records, tokenizer, max_len=MAX_LEN)\nval_ds = IntentDataset(val_records, tokenizer, max_len=MAX_LEN)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n\n# -----------------------\n# Model\n# -----------------------\nclass DebertaMaskedModel(nn.Module):\n    def __init__(self, base_model_name, num_primaries, num_secondaries, dropout=0.1):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(base_model_name)\n        hidden = self.encoder.config.hidden_size\n\n        # ---------- Primary Head ----------\n        self.primary_head = nn.Sequential(\n            nn.Linear(hidden, hidden),\n            nn.GELU(),\n            nn.LayerNorm(hidden),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, num_primaries)\n        )\n\n        # ---------- Secondary Head ----------\n        self.secondary_head = nn.Sequential(\n            nn.Linear(hidden, hidden * 2),\n            nn.GELU(),\n            nn.Dropout(dropout),\n\n            nn.Linear(hidden * 2, hidden),\n            nn.GELU(),\n            nn.LayerNorm(hidden),\n            nn.Dropout(dropout),\n\n            nn.Linear(hidden, num_secondaries)\n        )\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input_ids, attention_mask):\n        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        cls = out.last_hidden_state[:, 0, :]\n        cls = self.dropout(cls)\n        return self.primary_head(cls), self.secondary_head(cls)\n\n\ndevice = torch.device(DEVICE)\nmodel = DebertaMaskedModel(MODEL_NAME, num_primaries=len(primary_labels), num_secondaries=len(all_secondary)).to(device)\n\n# optimizer + scheduler\nno_decay = [\"bias\", \"LayerNorm.weight\"]\ngrouped = [\n    {\"params\": [p for n,p in model.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n    {\"params\": [p for n,p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n]\noptim = AdamW(grouped, lr=LR)\ntotal_steps = max(1, len(train_loader) * EPOCHS)\nscheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps=WARMUP_STEPS, num_training_steps=total_steps)\n\nprimary_loss_fn = nn.CrossEntropyLoss()\nsecondary_loss_fn = nn.CrossEntropyLoss()\n\n# -----------------------\n# Allowed mask helper\n# -----------------------\ndef build_allowed_mask_batch(primary_ids: torch.Tensor, speakers: List[str], num_secondary: int):\n    batch = primary_ids.cpu().tolist()\n    mask = torch.zeros((len(batch), num_secondary), dtype=torch.bool)\n    for i, p in enumerate(batch):\n        sp = speakers[i].lower()\n        allowed = allowed_map.get((int(p), sp), [])\n        if allowed:\n            mask[i, allowed] = True\n        else:\n            fallback = sec_agent if sp == \"agent\" else sec_customer\n            mask[i, [sec2id[s] for s in fallback]] = True\n    return mask.to(device)\n\n# -----------------------\n# Training loop\n# -----------------------\nbest_val_primary_acc = 0.0\n\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    running_loss = 0.0\n    pbar = tqdm(train_loader, desc=f\"Train Epoch {epoch}\")\n    for batch in pbar:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        speakers = batch[\"speakers\"]\n        prim_y = batch[\"primaries\"].to(device)\n        sec_y = batch[\"secondaries\"].to(device)\n\n        optim.zero_grad()\n        primary_logits, secondary_logits = model(input_ids, attention_mask)\n\n        loss_p = primary_loss_fn(primary_logits, prim_y)\n\n        allowed_mask = build_allowed_mask_batch(prim_y, speakers, num_secondary=len(all_secondary))\n        masked_sec_logits = secondary_logits.masked_fill(~allowed_mask, -1e9)\n        loss_s = secondary_loss_fn(masked_sec_logits, sec_y)\n\n        loss = loss_p + ALPHA_SEC * loss_s\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optim.step()\n        scheduler.step()\n\n        running_loss += loss.item()\n    # validation\n    model.eval()\n    prim_preds, prim_trues = [], []\n    sec_preds, sec_trues = [], []\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Validation\"):\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            speakers = batch[\"speakers\"]\n            prim_y = batch[\"primaries\"].to(device)\n            sec_y = batch[\"secondaries\"].to(device)\n\n            primary_logits, secondary_logits = model(input_ids, attention_mask)\n            prim_pred = torch.argmax(primary_logits, dim=1).cpu().tolist()\n            prim_preds.extend(prim_pred)\n            prim_trues.extend(prim_y.cpu().tolist())\n\n            allowed_mask = build_allowed_mask_batch(prim_y, speakers, num_secondary=len(all_secondary))\n            masked_sec_logits = secondary_logits.masked_fill(~allowed_mask, -1e9)\n            sec_pred = torch.argmax(masked_sec_logits, dim=1).cpu().tolist()\n            sec_preds.extend(sec_pred)\n            sec_trues.extend(sec_y.cpu().tolist())\n\n    prim_acc = accuracy_score(prim_trues, prim_preds) if prim_trues else 0.0\n    sec_acc = accuracy_score(sec_trues, sec_preds) if sec_trues else 0.0\n    print(f\"Epoch {epoch} VALID primary_acc={prim_acc:.4f} secondary_acc={sec_acc:.4f}\")\n\n    if prim_acc > best_val_primary_acc:\n        best_val_primary_acc = prim_acc\n        torch.save({\n            \"epoch\": epoch,\n            \"model_state\": model.state_dict(),\n            \"primary2id\": primary2id,\n            \"sec2id\": sec2id\n        }, os.path.join(OUTPUT_DIR, \"best_model.pt\"))\n        tokenizer.save_pretrained(OUTPUT_DIR)\n        print(\"Saved best model.\")\n\n# final save\ntorch.save(model.state_dict(), os.path.join(OUTPUT_DIR, \"final_model.pt\"))\ntokenizer.save_pretrained(OUTPUT_DIR)\nprint(\"Training finished. Best primary acc:\", best_val_primary_acc)\nprint(\"Saved final model & tokenizer to\", OUTPUT_DIR)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:07:06.985322Z","iopub.execute_input":"2025-12-02T16:07:06.985990Z","iopub.status.idle":"2025-12-02T16:47:33.301643Z","shell.execute_reply.started":"2025-12-02T16:07:06.985964Z","shell.execute_reply":"2025-12-02T16:47:33.300807Z"}},"outputs":[{"name":"stdout","text":"Loaded agent: 11583 customer: 11800\nValid records -> agent: 11583 customer: 11800 total: 23383\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train Epoch 1:   0%|          | 0/1170 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5681759a10e94cb389b870bf3f83ef3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/293 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf2de239b0ba49929dee777fa93c9734"}},"metadata":{}},{"name":"stdout","text":"Epoch 1 VALID primary_acc=0.4182 secondary_acc=0.5166\nSaved best model.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train Epoch 2:   0%|          | 0/1170 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c88ac4b46fa149858a05011c999f9759"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/293 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8e733068835424684bd4fa0287ec8b3"}},"metadata":{}},{"name":"stdout","text":"Epoch 2 VALID primary_acc=0.4432 secondary_acc=0.5602\nSaved best model.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train Epoch 3:   0%|          | 0/1170 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43afb939e66d480fb69cf29958c86bde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/293 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bfeb17c81ea454fb498e864e752bf99"}},"metadata":{}},{"name":"stdout","text":"Epoch 3 VALID primary_acc=0.4539 secondary_acc=0.5893\nSaved best model.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train Epoch 4:   0%|          | 0/1170 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cf8d9d23f5643cda1fec095c8292299"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/293 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc7ab44223e84d68886bc71d5650ddb7"}},"metadata":{}},{"name":"stdout","text":"Epoch 4 VALID primary_acc=0.4550 secondary_acc=0.5948\nSaved best model.\nTraining finished. Best primary acc: 0.45499251657045114\nSaved final model & tokenizer to /kaggle/working/deberta_debug_model_fixed\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"#!/usr/bin/env python3\n# continue_train_deberta.py\n\nimport os\nimport json\nimport random\nimport difflib\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import AdamW\nfrom transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\nfrom sklearn.metrics import accuracy_score\nfrom tqdm.auto import tqdm\n\n# -----------------------\n# CONFIG - edit if needed\n# -----------------------\nMODEL_NAME = \"microsoft/deberta-v3-base\"\nAGENT_FILE = \"/kaggle/input/primary-secondary-data-with-general/labeled_intent_dataset_agent_with_general.json\"\nCUSTOMER_FILE = \"/kaggle/input/primary-secondary-data-with-general/labeled_intent_dataset_customer_with_general.json\"\nTAXONOMY_PATH = \"/kaggle/input/taxonomy-json/taxonomy.json\"\nCHECKPOINT_PATH = \"/kaggle/working/deberta_debug_model_fixed/best_model.pt\"\nOUTPUT_DIR = \"/kaggle/working/deberta_continued_model\"\n\nBATCH_SIZE = 16\nMAX_LEN = 128\nLR = 1e-5  # Lower learning rate for fine-tuning\nEPOCHS = 8  # Additional 8 epochs\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nSEED = 42\nALPHA_SEC = 1.0\nWARMUP_STEPS = 50\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nrandom.seed(SEED)\ntorch.manual_seed(SEED)\n\n# -----------------------\n# Helpers\n# -----------------------\ndef load_json_list(path: str) -> List[dict]:\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n    if not isinstance(data, list):\n        raise ValueError(f\"{path} must contain a JSON array (list of objects).\")\n    return data\n\ndef normalize_str(s):\n    if s is None:\n        return \"\"\n    return str(s).strip()\n\ndef fuzzy_match_one(item: str, choices: List[str], cutoff: float = 0.6) -> str:\n    item_s = normalize_str(item)\n    if not item_s:\n        return None\n    for c in choices:\n        if item_s == c:\n            return c\n    for c in choices:\n        if item_s.lower() == c.lower():\n            return c\n    matches = difflib.get_close_matches(item_s, choices, n=1, cutoff=cutoff)\n    if matches:\n        return matches[0]\n    matches = difflib.get_close_matches(item_s.lower(), [c.lower() for c in choices], n=1, cutoff=cutoff)\n    if matches:\n        for c in choices:\n            if c.lower() == matches[0]:\n                return c\n    return None\n\n# -----------------------\n# Load taxonomy & build label maps\n# -----------------------\nwith open(TAXONOMY_PATH, \"r\", encoding=\"utf-8\") as f:\n    taxonomy = json.load(f)\n\nprimary_labels = sorted(list(taxonomy.keys()))\nprimary2id = {p: i for i, p in enumerate(primary_labels)}\nid2primary = {i: p for p, i in primary2id.items()}\n\nsec_agent = sorted({s for v in taxonomy.values() for s in v.get(\"agent\", [])})\nsec_customer = sorted({s for v in taxonomy.values() for s in v.get(\"user\", [])})\nall_secondary = sorted(list(set(sec_agent + sec_customer)))\nsec2id = {s: i for i, s in enumerate(all_secondary)}\nid2sec = {i: s for s, i in sec2id.items()}\n\nallowed_map: Dict[Tuple[int, str], List[int]] = {}\nfor p_name, p_id in primary2id.items():\n    a_children = taxonomy[p_name].get(\"agent\", [])\n    u_children = taxonomy[p_name].get(\"user\", [])\n    allowed_map[(p_id, \"agent\")] = [sec2id[s] for s in a_children if s in sec2id]\n    allowed_map[(p_id, \"customer\")] = [sec2id[s] for s in u_children if s in sec2id]\n\n# -----------------------\n# Load raw data files\n# -----------------------\nagent_raw = load_json_list(AGENT_FILE)\ncustomer_raw = load_json_list(CUSTOMER_FILE)\nprint(\"Loaded agent:\", len(agent_raw), \"customer:\", len(customer_raw))\n\n# -----------------------\n# Fuzzy map primary & secondary labels\n# -----------------------\ndef map_records(records: List[dict]) -> List[dict]:\n    mapped = []\n    for r in records:\n        rr = dict(r)\n        prim = normalize_str(rr.get(\"primary_intent\") or \"\")\n        sec = normalize_str(rr.get(\"secondary_intent\") or \"\")\n        sp = normalize_str(rr.get(\"speaker\") or rr.get(\"original_speaker\") or \"\").lower()\n\n        mapped_primary = fuzzy_match_one(prim, primary_labels, cutoff=0.6)\n        if mapped_primary:\n            rr[\"primary_intent\"] = mapped_primary\n        pool = sec_agent if sp == \"agent\" else sec_customer\n        mapped_secondary = fuzzy_match_one(sec, pool, cutoff=0.55)\n        if mapped_secondary:\n            rr[\"secondary_intent\"] = mapped_secondary\n\n        if sp.startswith(\"agent\"):\n            rr[\"speaker\"] = \"agent\"\n        elif sp.startswith(\"cust\") or sp.startswith(\"customer\") or sp.startswith(\"user\"):\n            rr[\"speaker\"] = \"customer\"\n        else:\n            rr[\"speaker\"] = sp if sp else rr.get(\"speaker\",\"\").lower() or \"customer\"\n        mapped.append(rr)\n    return mapped\n\nagent_mapped = map_records(agent_raw)\ncustomer_mapped = map_records(customer_raw)\n\n# -----------------------\n# Filter valid records\n# -----------------------\ndef is_valid_record(r):\n    return (\n        (r.get(\"primary_intent\") in primary2id)\n        and (r.get(\"secondary_intent\") in sec2id)\n        and (str(r.get(\"speaker\",\"\")).lower() in (\"agent\",\"customer\"))\n        and ( (r.get(\"text\") or r.get(\"full_text\") or \"\").strip() != \"\" )\n    )\n\nagent_valid = [r for r in agent_mapped if is_valid_record(r)]\ncustomer_valid = [r for r in customer_mapped if is_valid_record(r)]\nall_records = agent_valid + customer_valid\nrandom.shuffle(all_records)\nprint(\"Valid records -> agent:\", len(agent_valid), \"customer:\", len(customer_valid), \"total:\", len(all_records))\n\nif len(all_records) == 0:\n    raise RuntimeError(\"No valid records after mapping/filtering.\")\n\n# -----------------------\n# Tokenizer & Dataset\n# -----------------------\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n\nclass IntentDataset(Dataset):\n    def __init__(self, records: List[dict], tokenizer, max_len=128):\n        self.records = records\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.records)\n\n    def __getitem__(self, idx):\n        r = self.records[idx]\n        text = r.get(\"text\") or r.get(\"full_text\") or \"\"\n        enc = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n        primary_id = primary2id.get(r.get(\"primary_intent\"), -1)\n        secondary_id = sec2id.get(r.get(\"secondary_intent\"), -1)\n        return {\n            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n            \"speaker\": r.get(\"speaker\",\"\").lower(),\n            \"primary_id\": primary_id,\n            \"secondary_id\": secondary_id\n        }\n\ndef collate_fn(batch):\n    input_ids = torch.stack([b[\"input_ids\"] for b in batch])\n    attention_mask = torch.stack([b[\"attention_mask\"] for b in batch])\n    speakers = [b[\"speaker\"] for b in batch]\n    primaries = torch.tensor([b[\"primary_id\"] for b in batch], dtype=torch.long)\n    secondaries = torch.tensor([b[\"secondary_id\"] for b in batch], dtype=torch.long)\n    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"speakers\": speakers, \"primaries\": primaries, \"secondaries\": secondaries}\n\n# train/val split\nsplit_idx = int(0.8 * len(all_records))\ntrain_records = all_records[:split_idx]\nval_records = all_records[split_idx:]\n\ntrain_ds = IntentDataset(train_records, tokenizer, max_len=MAX_LEN)\nval_ds = IntentDataset(val_records, tokenizer, max_len=MAX_LEN)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n\n# -----------------------\n# Model\n# -----------------------\nclass DebertaMaskedModel(nn.Module):\n    def __init__(self, base_model_name, num_primaries, num_secondaries, dropout=0.1):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(base_model_name)\n        hidden = self.encoder.config.hidden_size\n\n        self.primary_head = nn.Sequential(\n            nn.Linear(hidden, hidden),\n            nn.GELU(),\n            nn.LayerNorm(hidden),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, num_primaries)\n        )\n\n        self.secondary_head = nn.Sequential(\n            nn.Linear(hidden, hidden * 2),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden * 2, hidden),\n            nn.GELU(),\n            nn.LayerNorm(hidden),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, num_secondaries)\n        )\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input_ids, attention_mask):\n        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        cls = out.last_hidden_state[:, 0, :]\n        cls = self.dropout(cls)\n        return self.primary_head(cls), self.secondary_head(cls)\n\n\ndevice = torch.device(DEVICE)\nmodel = DebertaMaskedModel(MODEL_NAME, num_primaries=len(primary_labels), num_secondaries=len(all_secondary)).to(device)\n\n# -----------------------\n# Load checkpoint\n# -----------------------\nprint(f\"Loading checkpoint from {CHECKPOINT_PATH}\")\ncheckpoint = torch.load(CHECKPOINT_PATH, map_location=device)\nmodel.load_state_dict(checkpoint[\"model_state\"])\nstarting_epoch = checkpoint.get(\"epoch\", 0)\nprint(f\"Loaded model from epoch {starting_epoch}\")\n\n# Verify label mappings match\nif checkpoint.get(\"primary2id\") != primary2id:\n    print(\"WARNING: primary2id mismatch between checkpoint and current taxonomy!\")\nif checkpoint.get(\"sec2id\") != sec2id:\n    print(\"WARNING: sec2id mismatch between checkpoint and current taxonomy!\")\n\n# -----------------------\n# Optimizer + Scheduler\n# -----------------------\nno_decay = [\"bias\", \"LayerNorm.weight\"]\ngrouped = [\n    {\"params\": [p for n,p in model.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n    {\"params\": [p for n,p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n]\noptim = AdamW(grouped, lr=LR)\ntotal_steps = max(1, len(train_loader) * EPOCHS)\nscheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps=WARMUP_STEPS, num_training_steps=total_steps)\n\nprimary_loss_fn = nn.CrossEntropyLoss()\nsecondary_loss_fn = nn.CrossEntropyLoss()\n\n# -----------------------\n# Allowed mask helper\n# -----------------------\ndef build_allowed_mask_batch(primary_ids: torch.Tensor, speakers: List[str], num_secondary: int):\n    batch = primary_ids.cpu().tolist()\n    mask = torch.zeros((len(batch), num_secondary), dtype=torch.bool)\n    for i, p in enumerate(batch):\n        sp = speakers[i].lower()\n        allowed = allowed_map.get((int(p), sp), [])\n        if allowed:\n            mask[i, allowed] = True\n        else:\n            fallback = sec_agent if sp == \"agent\" else sec_customer\n            mask[i, [sec2id[s] for s in fallback]] = True\n    return mask.to(device)\n\n# -----------------------\n# Training loop\n# -----------------------\nbest_val_primary_acc = 0.0\n\nfor epoch in range(1, EPOCHS + 1):\n    actual_epoch = starting_epoch + epoch\n    model.train()\n    running_loss = 0.0\n    pbar = tqdm(train_loader, desc=f\"Train Epoch {actual_epoch}\")\n    \n    for batch in pbar:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        speakers = batch[\"speakers\"]\n        prim_y = batch[\"primaries\"].to(device)\n        sec_y = batch[\"secondaries\"].to(device)\n\n        optim.zero_grad()\n        primary_logits, secondary_logits = model(input_ids, attention_mask)\n\n        loss_p = primary_loss_fn(primary_logits, prim_y)\n\n        allowed_mask = build_allowed_mask_batch(prim_y, speakers, num_secondary=len(all_secondary))\n        masked_sec_logits = secondary_logits.masked_fill(~allowed_mask, -1e9)\n        loss_s = secondary_loss_fn(masked_sec_logits, sec_y)\n\n        loss = loss_p + ALPHA_SEC * loss_s\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optim.step()\n        scheduler.step()\n\n        running_loss += loss.item()\n        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n    \n    avg_train_loss = running_loss / len(train_loader)\n    \n    # Validation\n    model.eval()\n    prim_preds, prim_trues = [], []\n    sec_preds, sec_trues = [], []\n    val_loss = 0.0\n    \n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Validation\"):\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            speakers = batch[\"speakers\"]\n            prim_y = batch[\"primaries\"].to(device)\n            sec_y = batch[\"secondaries\"].to(device)\n\n            primary_logits, secondary_logits = model(input_ids, attention_mask)\n            \n            loss_p = primary_loss_fn(primary_logits, prim_y)\n            allowed_mask = build_allowed_mask_batch(prim_y, speakers, num_secondary=len(all_secondary))\n            masked_sec_logits = secondary_logits.masked_fill(~allowed_mask, -1e9)\n            loss_s = secondary_loss_fn(masked_sec_logits, sec_y)\n            val_loss += (loss_p + ALPHA_SEC * loss_s).item()\n            \n            prim_pred = torch.argmax(primary_logits, dim=1).cpu().tolist()\n            prim_preds.extend(prim_pred)\n            prim_trues.extend(prim_y.cpu().tolist())\n\n            sec_pred = torch.argmax(masked_sec_logits, dim=1).cpu().tolist()\n            sec_preds.extend(sec_pred)\n            sec_trues.extend(sec_y.cpu().tolist())\n\n    avg_val_loss = val_loss / len(val_loader)\n    prim_acc = accuracy_score(prim_trues, prim_preds) if prim_trues else 0.0\n    sec_acc = accuracy_score(sec_trues, sec_preds) if sec_trues else 0.0\n    \n    print(f\"\\nEpoch {actual_epoch} Summary:\")\n    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n    print(f\"  Val Loss: {avg_val_loss:.4f}\")\n    print(f\"  Primary Acc: {prim_acc:.4f}\")\n    print(f\"  Secondary Acc: {sec_acc:.4f}\")\n\n    if prim_acc > best_val_primary_acc:\n        best_val_primary_acc = prim_acc\n        torch.save({\n            \"epoch\": actual_epoch,\n            \"model_state\": model.state_dict(),\n            \"primary2id\": primary2id,\n            \"sec2id\": sec2id,\n            \"primary_acc\": prim_acc,\n            \"secondary_acc\": sec_acc\n        }, os.path.join(OUTPUT_DIR, \"best_model.pt\"))\n        tokenizer.save_pretrained(OUTPUT_DIR)\n        print(f\"  ✓ Saved new best model with primary acc: {prim_acc:.4f}\")\n\n# Final save\ntorch.save({\n    \"epoch\": starting_epoch + EPOCHS,\n    \"model_state\": model.state_dict(),\n    \"primary2id\": primary2id,\n    \"sec2id\": sec2id\n}, os.path.join(OUTPUT_DIR, \"final_model.pt\"))\ntokenizer.save_pretrained(OUTPUT_DIR)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Training finished!\")\nprint(f\"Best primary accuracy: {best_val_primary_acc:.4f}\")\nprint(f\"Final model saved to: {OUTPUT_DIR}\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T17:28:11.116154Z","iopub.execute_input":"2025-12-02T17:28:11.116472Z","iopub.status.idle":"2025-12-02T18:48:57.857206Z","shell.execute_reply.started":"2025-12-02T17:28:11.116450Z","shell.execute_reply":"2025-12-02T18:48:57.856321Z"}},"outputs":[{"name":"stdout","text":"Loaded agent: 11583 customer: 11800\nValid records -> agent: 11583 customer: 11800 total: 23383\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Loading checkpoint from /kaggle/working/deberta_debug_model_fixed/best_model.pt\nLoaded model from epoch 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train Epoch 5:   0%|          | 0/1170 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f99599e6257f466383bc38a9f32f9388"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/293 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c566708196149688f96ef6afe6d9a4b"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 5 Summary:\n  Train Loss: 2.2172\n  Val Loss: 2.8732\n  Primary Acc: 0.4529\n  Secondary Acc: 0.5948\n  ✓ Saved new best model with primary acc: 0.4529\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train Epoch 6:   0%|          | 0/1170 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0db942b210f84531ba68cb4537fd13ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/293 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35cc4250bdcf451f9665f79ea069b607"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 6 Summary:\n  Train Loss: 1.8931\n  Val Loss: 2.9371\n  Primary Acc: 0.4458\n  Secondary Acc: 0.5974\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train Epoch 7:   0%|          | 0/1170 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e0d2261be12453f872dc17f9d76b813"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/293 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bb49bdeb13b450ca7a3a35196dc6d3f"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 7 Summary:\n  Train Loss: 1.7558\n  Val Loss: 3.0370\n  Primary Acc: 0.4419\n  Secondary Acc: 0.5989\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train Epoch 8:   0%|          | 0/1170 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d0dda0278b24276a98b109fc0057e9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/293 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c2e2ff63446441bbfff302ce125eb97"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 8 Summary:\n  Train Loss: 1.7690\n  Val Loss: 3.1021\n  Primary Acc: 0.4383\n  Secondary Acc: 0.5993\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train Epoch 9:   0%|          | 0/1170 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9d5ee696bb5403ab42cc63e3057c595"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/293 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b0f1c54354646e8a774e5002aafb9d7"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 9 Summary:\n  Train Loss: 1.7566\n  Val Loss: 3.1542\n  Primary Acc: 0.4409\n  Secondary Acc: 0.5980\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train Epoch 10:   0%|          | 0/1170 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f11484da4d5b405594ae00e7feeb682a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/293 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47b3f11e9ae14dfeab2f0fbb56e37a34"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 10 Summary:\n  Train Loss: 1.6414\n  Val Loss: 3.1919\n  Primary Acc: 0.4366\n  Secondary Acc: 0.6008\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train Epoch 11:   0%|          | 0/1170 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0de439aa7a374b17a94bfc16dda01184"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/293 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e72fa73a30049ddbb9b7a2e7a1cddbd"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 11 Summary:\n  Train Loss: 1.5608\n  Val Loss: 3.2175\n  Primary Acc: 0.4364\n  Secondary Acc: 0.6021\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Train Epoch 12:   0%|          | 0/1170 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65a6ad12ad7c4a38a726cbc4d54cb9fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation:   0%|          | 0/293 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08464bb774c34c4888f3cbdb729130df"}},"metadata":{}},{"name":"stdout","text":"\nEpoch 12 Summary:\n  Train Loss: 1.4944\n  Val Loss: 3.2312\n  Primary Acc: 0.4353\n  Secondary Acc: 0.5982\n\n============================================================\nTraining finished!\nBest primary accuracy: 0.4529\nFinal model saved to: /kaggle/working/deberta_continued_model\n============================================================\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"#!/usr/bin/env python3\n# test_model_inference.py\n\nimport os\nimport json\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModel\nfrom typing import Dict, List, Tuple\n\n# -----------------------\n# CONFIG\n# -----------------------\nMODEL_NAME = \"microsoft/deberta-v3-base\"\nCHECKPOINT_PATH = \"/kaggle/working/deberta_continued_model/best_model.pt\"  # or use deberta_debug_model_fixed/best_model.pt\nTAXONOMY_PATH = \"/kaggle/input/taxonomy-json/taxonomy.json\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# -----------------------\n# Model Definition (same as training)\n# -----------------------\nclass DebertaMaskedModel(nn.Module):\n    def __init__(self, base_model_name, num_primaries, num_secondaries, dropout=0.1):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(base_model_name)\n        hidden = self.encoder.config.hidden_size\n\n        self.primary_head = nn.Sequential(\n            nn.Linear(hidden, hidden),\n            nn.GELU(),\n            nn.LayerNorm(hidden),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, num_primaries)\n        )\n\n        self.secondary_head = nn.Sequential(\n            nn.Linear(hidden, hidden * 2),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden * 2, hidden),\n            nn.GELU(),\n            nn.LayerNorm(hidden),\n            nn.Dropout(dropout),\n            nn.Linear(hidden, num_secondaries)\n        )\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input_ids, attention_mask):\n        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        cls = out.last_hidden_state[:, 0, :]\n        cls = self.dropout(cls)\n        return self.primary_head(cls), self.secondary_head(cls)\n\n# -----------------------\n# Load Model & Taxonomy\n# -----------------------\nprint(\"Loading checkpoint...\")\ncheckpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE, weights_only=False)\n\nprimary2id = checkpoint[\"primary2id\"]\nsec2id = checkpoint[\"sec2id\"]\nid2primary = {i: p for p, i in primary2id.items()}\nid2sec = {i: s for s, i in sec2id.items()}\n\nprint(f\"Loaded model from epoch {checkpoint.get('epoch', 'unknown')}\")\nprint(f\"Primary intents: {len(primary2id)}\")\nprint(f\"Secondary intents: {len(sec2id)}\")\n\n# Load taxonomy for allowed mappings\nwith open(TAXONOMY_PATH, \"r\", encoding=\"utf-8\") as f:\n    taxonomy = json.load(f)\n\n# Build allowed secondary intents per primary\nallowed_map: Dict[Tuple[int, str], List[int]] = {}\nfor p_name, p_id in primary2id.items():\n    a_children = taxonomy[p_name].get(\"agent\", [])\n    u_children = taxonomy[p_name].get(\"user\", [])\n    allowed_map[(p_id, \"agent\")] = [sec2id[s] for s in a_children if s in sec2id]\n    allowed_map[(p_id, \"customer\")] = [sec2id[s] for s in u_children if s in sec2id]\n\n# Initialize model\ndevice = torch.device(DEVICE)\nmodel = DebertaMaskedModel(\n    MODEL_NAME, \n    num_primaries=len(primary2id), \n    num_secondaries=len(sec2id)\n).to(device)\n\nmodel.load_state_dict(checkpoint[\"model_state\"])\nmodel.eval()\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\nprint(\"Model loaded successfully!\\n\")\n\n# -----------------------\n# Inference Function\n# -----------------------\ndef predict(text: str, speaker: str = \"customer\", max_len: int = 128, top_k: int = 3):\n    \"\"\"\n    Predict primary and secondary intents for a given text.\n    \n    Args:\n        text: Input text to classify\n        speaker: \"agent\" or \"customer\"\n        max_len: Maximum token length\n        top_k: Number of top predictions to return\n    \"\"\"\n    speaker = speaker.lower()\n    if speaker not in [\"agent\", \"customer\"]:\n        speaker = \"customer\"\n    \n    # Tokenize\n    encoding = tokenizer(\n        text,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=max_len,\n        return_tensors=\"pt\"\n    )\n    \n    input_ids = encoding[\"input_ids\"].to(device)\n    attention_mask = encoding[\"attention_mask\"].to(device)\n    \n    # Predict\n    with torch.no_grad():\n        primary_logits, secondary_logits = model(input_ids, attention_mask)\n    \n    # Get primary predictions\n    primary_probs = torch.softmax(primary_logits, dim=1)[0]\n    primary_pred_id = torch.argmax(primary_probs).item()\n    primary_pred = id2primary[primary_pred_id]\n    primary_conf = primary_probs[primary_pred_id].item()\n    \n    # Get top-k primary predictions\n    top_primary_probs, top_primary_ids = torch.topk(primary_probs, min(top_k, len(primary2id)))\n    top_primaries = [\n        (id2primary[idx.item()], prob.item()) \n        for idx, prob in zip(top_primary_ids, top_primary_probs)\n    ]\n    \n    # Get secondary predictions (masked by allowed intents)\n    allowed_secondary_ids = allowed_map.get((primary_pred_id, speaker), [])\n    \n    if allowed_secondary_ids:\n        # Create mask for allowed secondaries\n        mask = torch.zeros(len(sec2id), dtype=torch.bool, device=device)\n        mask[allowed_secondary_ids] = True\n        masked_logits = secondary_logits[0].masked_fill(~mask, -1e9)\n    else:\n        masked_logits = secondary_logits[0]\n    \n    secondary_probs = torch.softmax(masked_logits, dim=0)\n    secondary_pred_id = torch.argmax(secondary_probs).item()\n    secondary_pred = id2sec[secondary_pred_id]\n    secondary_conf = secondary_probs[secondary_pred_id].item()\n    \n    # Get top-k secondary predictions\n    top_secondary_probs, top_secondary_ids = torch.topk(secondary_probs, min(top_k, len(sec2id)))\n    top_secondaries = [\n        (id2sec[idx.item()], prob.item()) \n        for idx, prob in zip(top_secondary_ids, top_secondary_probs)\n        if prob.item() > -1e8  # Filter out masked values\n    ]\n    \n    return {\n        \"text\": text,\n        \"speaker\": speaker,\n        \"primary_intent\": primary_pred,\n        \"primary_confidence\": primary_conf,\n        \"secondary_intent\": secondary_pred,\n        \"secondary_confidence\": secondary_conf,\n        \"top_primary_predictions\": top_primaries,\n        \"top_secondary_predictions\": top_secondaries,\n        \"allowed_secondaries\": [id2sec[i] for i in allowed_secondary_ids] if allowed_secondary_ids else []\n    }\n\ndef print_prediction(result: dict):\n    \"\"\"Pretty print prediction results\"\"\"\n    print(\"=\"*70)\n    print(f\"Input: {result['text']}\")\n    print(f\"Speaker: {result['speaker'].upper()}\")\n    print(\"-\"*70)\n    print(f\"PRIMARY INTENT: {result['primary_intent']} (confidence: {result['primary_confidence']:.2%})\")\n    print(f\"SECONDARY INTENT: {result['secondary_intent']} (confidence: {result['secondary_confidence']:.2%})\")\n    print(\"-\"*70)\n    \n    print(\"\\nTop Primary Predictions:\")\n    for i, (intent, conf) in enumerate(result['top_primary_predictions'], 1):\n        print(f\"  {i}. {intent}: {conf:.2%}\")\n    \n    print(\"\\nTop Secondary Predictions:\")\n    for i, (intent, conf) in enumerate(result['top_secondary_predictions'], 1):\n        print(f\"  {i}. {intent}: {conf:.2%}\")\n    \n    if result['allowed_secondaries']:\n        print(f\"\\nAllowed secondaries for this primary: {len(result['allowed_secondaries'])}\")\n    print(\"=\"*70 + \"\\n\")\n\n# -----------------------\n# Interactive Testing\n# -----------------------\ndef interactive_mode():\n    \"\"\"Run interactive testing mode\"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"INTERACTIVE TESTING MODE\")\n    print(\"=\"*70)\n    print(\"Enter 'quit' or 'exit' to stop\")\n    print(\"Enter 'examples' to see example inputs\\n\")\n    \n    while True:\n        text = input(\"Enter text to classify: \").strip()\n        \n        if text.lower() in ['quit', 'exit', 'q']:\n            print(\"Exiting...\")\n            break\n        \n        if text.lower() == 'examples':\n            print(\"\\nExample inputs:\")\n            print(\"  - I want to cancel my subscription\")\n            print(\"  - What's my account balance?\")\n            print(\"  - Let me transfer you to a specialist\")\n            print(\"  - I need help with my order\")\n            print(\"  - Thank you for your help!\\n\")\n            continue\n        \n        if not text:\n            print(\"Please enter some text.\\n\")\n            continue\n        \n        speaker = input(\"Speaker (agent/customer) [default: customer]: \").strip().lower()\n        if not speaker:\n            speaker = \"customer\"\n        \n        result = predict(text, speaker)\n        print()\n        print_prediction(result)\n\n# -----------------------\n# Batch Testing\n# -----------------------\ndef batch_test(examples: List[Tuple[str, str]]):\n    \"\"\"Test multiple examples at once\"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"BATCH TESTING\")\n    print(\"=\"*70 + \"\\n\")\n    \n    for text, speaker in examples:\n        result = predict(text, speaker)\n        print_prediction(result)\n\n# -----------------------\n# Main\n# -----------------------\nif __name__ == \"__main__\":\n    # Example test cases\n    test_examples = [\n        (\"I want to cancel my subscription\", \"customer\"),\n        (\"Let me check your account details\", \"agent\"),\n        (\"What's my current balance?\", \"customer\"),\n        (\"I'll transfer you to our billing department\", \"agent\"),\n        (\"Thank you so much for your help!\", \"customer\"),\n    ]\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"MODEL TESTING SCRIPT\")\n    print(\"=\"*70)\n    print(\"\\nChoose a mode:\")\n    print(\"1. Interactive mode (enter custom inputs)\")\n    print(\"2. Batch test with example inputs\")\n    print(\"3. Single prediction\")\n    \n    choice = input(\"\\nEnter choice (1/2/3) [default: 1]: \").strip()\n    \n    if choice == \"2\":\n        batch_test(test_examples)\n    elif choice == \"3\":\n        text = input(\"Enter text: \").strip()\n        speaker = input(\"Enter speaker (agent/customer) [default: customer]: \").strip().lower() or \"customer\"\n        result = predict(text, speaker)\n        print()\n        print_prediction(result)\n    else:\n        interactive_mode()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T18:57:36.415425Z","iopub.execute_input":"2025-12-02T18:57:36.416007Z","iopub.status.idle":"2025-12-02T18:59:06.253118Z","shell.execute_reply.started":"2025-12-02T18:57:36.415982Z","shell.execute_reply":"2025-12-02T18:59:06.252537Z"}},"outputs":[{"name":"stdout","text":"Loading checkpoint...\nLoaded model from epoch 5\nPrimary intents: 17\nSecondary intents: 98\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Model loaded successfully!\n\n\n======================================================================\nMODEL TESTING SCRIPT\n======================================================================\n\nChoose a mode:\n1. Interactive mode (enter custom inputs)\n2. Batch test with example inputs\n3. Single prediction\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter choice (1/2/3) [default: 1]:  2\n"},{"name":"stdout","text":"\n======================================================================\nBATCH TESTING\n======================================================================\n\n======================================================================\nInput: I want to cancel my subscription\nSpeaker: CUSTOMER\n----------------------------------------------------------------------\nPRIMARY INTENT: Cancellation_Policy (confidence: 83.30%)\nSECONDARY INTENT: Inquire_About_Cancellation_Options (confidence: 76.43%)\n----------------------------------------------------------------------\n\nTop Primary Predictions:\n  1. Cancellation_Policy: 83.30%\n  2. Service_Switching: 8.65%\n  3. Billing & Refunds: 1.52%\n\nTop Secondary Predictions:\n  1. Inquire_About_Cancellation_Options: 76.43%\n  2. Ask_About_Reinstatement: 13.11%\n  3. Request_Policy_Clarification: 10.46%\n\nAllowed secondaries for this primary: 3\n======================================================================\n\n======================================================================\nInput: Let me check your account details\nSpeaker: AGENT\n----------------------------------------------------------------------\nPRIMARY INTENT: General_Conversation (confidence: 31.78%)\nSECONDARY INTENT: Ask_for_Clarification (confidence: 49.91%)\n----------------------------------------------------------------------\n\nTop Primary Predictions:\n  1. General_Conversation: 31.78%\n  2. Technical_Support: 17.00%\n  3. Fraud_Reporting: 14.37%\n\nTop Secondary Predictions:\n  1. Ask_for_Clarification: 49.91%\n  2. Acknowledge_or_Confirm: 29.41%\n  3. Closing_or_Goodbye: 10.37%\n\nAllowed secondaries for this primary: 5\n======================================================================\n\n======================================================================\nInput: What's my current balance?\nSpeaker: CUSTOMER\n----------------------------------------------------------------------\nPRIMARY INTENT: Credit_Limit_Adjustment (confidence: 94.77%)\nSECONDARY INTENT: Request_Credit_Limit_Increase (confidence: 57.21%)\n----------------------------------------------------------------------\n\nTop Primary Predictions:\n  1. Credit_Limit_Adjustment: 94.77%\n  2. Billing & Refunds: 1.50%\n  3. Technical_Support: 1.29%\n\nTop Secondary Predictions:\n  1. Request_Credit_Limit_Increase: 57.21%\n  2. Question_Approval: 37.74%\n  3. Inquire_About_Policies: 5.05%\n\nAllowed secondaries for this primary: 3\n======================================================================\n\n======================================================================\nInput: I'll transfer you to our billing department\nSpeaker: AGENT\n----------------------------------------------------------------------\nPRIMARY INTENT: Billing & Refunds (confidence: 41.92%)\nSECONDARY INTENT: Investigate_Billing_Issue (confidence: 62.61%)\n----------------------------------------------------------------------\n\nTop Primary Predictions:\n  1. Billing & Refunds: 41.92%\n  2. Order_Delivery: 23.92%\n  3. Product_Defect_Resolution: 10.47%\n\nTop Secondary Predictions:\n  1. Investigate_Billing_Issue: 62.61%\n  2. Process_Refund: 26.80%\n  3. Deny_Request_Policy: 5.60%\n\nAllowed secondaries for this primary: 4\n======================================================================\n\n======================================================================\nInput: Thank you so much for your help!\nSpeaker: CUSTOMER\n----------------------------------------------------------------------\nPRIMARY INTENT: General_Conversation (confidence: 39.86%)\nSECONDARY INTENT: Acknowledge_Confirm (confidence: 75.04%)\n----------------------------------------------------------------------\n\nTop Primary Predictions:\n  1. General_Conversation: 39.86%\n  2. Billing & Refunds: 7.86%\n  3. Service_Switching: 6.42%\n\nTop Secondary Predictions:\n  1. Acknowledge_Confirm: 75.04%\n  2. Hold/Wait: 13.17%\n  3. Ask_for_Clarification: 5.11%\n\nAllowed secondaries for this primary: 5\n======================================================================\n\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}